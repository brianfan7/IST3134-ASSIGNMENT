#steps after importing cleanedtext.txt using WINSCP
#connect to hadoop user instance

cd /home/ubuntu/

#copy cleanedtext to HDFS
hadoop fs -put cleanedtext.txt cleanedtext.txt

start-all.sh
cp cleanedtext.txt ~/IST3134
cd ~/IST3134

#create mapper.py and add the following code
nano mapper.py
" 
import sys
#reads input from the standard input line by line.
#split into words, store in words
for line in sys.stdin:
    line = line.strip()
    words = line.split()
    for word in words:
        print('%s\t%s' % (word, 1)) 
"

#create reducer.py
nano reducer.py
"
from operator import itemgetter
import sys

current_word = None
current_count = 0
word_count_list = []

#loop iterates over each line of input received from the standard input
#split word and count
#convert count into an integer
for line in sys.stdin:
    line = line.strip()
    word, count = line.split('\t', 1)
    count = int(count)

#checks if the current word is the same as the word just read from input.
#update the current_word and current_count variables to the new word and count, respectively.
    if current_word == word:
        current_count += count
    else:
        if current_word:
            word_count_list.append((current_word, current_count))
        current_word = word
        current_count = count

#handle the case when the last word of the input has been processed. 
#if the current word is the same as the word being read, add to word_count_list.
if current_word == word:
    word_count_list.append((current_word, current_count))

#Sort the word counts in descending order
sorted_word_count = sorted(word_count_list, key=itemgetter(1), reverse=True)

#output the top 100 most counted words
for i in range(min(100, len(sorted_word_count))):
    word, count = sorted_word_count[i]
    print(f"{word}\t{count}")
"

#launch mapreduce
mapred streaming -files mapper.py,reducer.py -input cleanedtext.txt -output oc -mapper "python3 mapper.py" -reducer "python3 reducer.py"

hadoop fs cat /user/hadoop/pc/part-00000
