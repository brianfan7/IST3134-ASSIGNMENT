#steps after importing cleanedtext.txt using WINSCP
#connect to hadoop user instance

cd /home/ubuntu/

#copy cleanedtext to HDFS
hadoop fs -put cleanedtext.txt cleanedtext.txt

start-all.sh
cp cleanedtext.txt ~/IST3134
cd ~/IST3134

#create mapper.py and add the following code
nano mapper.py
" 
import sys
#reads input from the standard input line by line.
#split into words, store in words
for line in sys.stdin:
    line = line.strip()
    words = line.split()
    for word in words:
        print(f"{word}\t1")
"

#create reducer.py
nano reducer.py
"
import sys

word_count = {}

#Read input from standard input line by line
for line in sys.stdin:
    line = line.strip()
    word, count = line.split('\t', 1)
    count = int(count)

    # Increment count for each word
    if word in word_count:
        word_count[word] += count
    else:
        word_count[word] = count

#Sort the word counts in descending order
sorted_word_count = sorted(word_count.items(), key=lambda x: x[1], reverse=True)

# Output the top 100 most counted words
for i in range(min(100, len(sorted_word_count))):
    word, count = sorted_word_count[i]
    print(f"{word}\t{count}")
"
#set execution permisions
$ chmod +x mapper.py
$ chmod +x reducer.py

#launch map reduce
mapred streaming -files mapper.py,reducer.py -input cleanedtext.txt -output oc -mapper "python3 mapper.py" -reducer "python3 reducer.py"
hadoop fs -cat /user/hadoop/pc/part-00000
