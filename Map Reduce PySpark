#Steps after importing cleanedtext.txt using WINSCP
#connect to hadoop user instance

#Start hadoop cluster
start-all.sh
cd /home/ubuntu/

#Copy cleantext6x.txt to HDFS and IST3134
hadoop fs -put cleantext6x.txt cleantext6x.txt
cp cleantext6x.txt ~/IST3134

cd ~/IST3134

#Create python file
nano sparkcount.py

#Add folling to python
"
#Import the necessary libraries
from pyspark.sql import SparkSession
import nltk
import re

#Create a SparkSession
spark = SparkSession.builder\
  .master("local[*]")\
  .appName("WordCount")\
  #Increase the memory capicity
  .config("spark.driver.memory", "4g") \
  .config("spark.executor.memory", "4g") \
  .getOrCreate()

#Obtain spark context
sc=spark.sparkContext

#Load and clean text data
textclean = sc.textFile("cleantext6x.txt")
textclean = textclean.map(lambda x: re.sub(r'\t', '', x))
textclean = textkclean.map(lambda x: re.sub(r'[^\w\s]', '', x))

#Word count calculation
textcount = textclean.map(lambda word: (word, 1))
textwc = textcount.reduceByKey(lambda x , y : (x+y))

#Sort and select top 100 count words
textsort = textwc.sortBy(lambda y:-y[1]).take(100)
rdd = spark.sparkContext.parallelize(textsort)

#Save file
rdd.saveAsTextFile('/user/hadoop/testcompare')

#Stop spark context
sc.stop()
"

#Run the code with time function
time python3 sparkcount.py
