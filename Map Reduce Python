#Steps after importing cleanedtext.txt using WINSCP
#Connect to hadoop user instance

cd /home/ubuntu/

#Copy cleanedtext to HDFS
hadoop fs -put cleanedtext.txt cleanedtext.txt

start-all.sh
cp cleanedtext.txt ~/IST3134
cd ~/IST3134

#Create mapper.py and add the following code
nano mapper.py
" 
import sys
#Reads input from the standard input line by line.
#Split into words, store in words
for line in sys.stdin:
    line = line.strip()
    words = line.split()
    for word in words:
        print(f"{word}\t1")
"

#Create reducer.py
nano reducer.py
"
import sys
from collections import defaultdict

word_count = defaultdict(int)

#Read input from standard input line by line
for line in sys.stdin:
    line = line.strip()
    word, count = line.split('\t', 1)
    count = int(count)

    #Increment count for each word
    word_count[word] += count

#Output the top 100 most counted words
for word, count in sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:100]:
    print(f"{word}\t{count}")
"

#Set execution permisions
$ chmod +x mapper.py
$ chmod +x reducer.py

#Launch MapReduce
mapred streaming -files mapper.py,reducer.py -input cleanedtext.txt -output oc -mapper "python3 mapper.py" -reducer "python3 reducer.py"
hadoop fs -cat /user/hadoop/pc/part-00000
